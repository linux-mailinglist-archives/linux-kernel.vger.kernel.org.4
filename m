Return-Path: <linux-kernel-owner@vger.kernel.org>
X-Original-To: lists+linux-kernel@lfdr.de
Delivered-To: lists+linux-kernel@lfdr.de
Received: from out1.vger.email (out1.vger.email [IPv6:2620:137:e000::1:20])
	by mail.lfdr.de (Postfix) with ESMTP id 6F64F5F899A
	for <lists+linux-kernel@lfdr.de>; Sun,  9 Oct 2022 08:14:20 +0200 (CEST)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229895AbiJIGOR (ORCPT <rfc822;lists+linux-kernel@lfdr.de>);
        Sun, 9 Oct 2022 02:14:17 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:39794 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S229808AbiJIGOO (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Sun, 9 Oct 2022 02:14:14 -0400
Received: from dggsgout11.his.huawei.com (unknown [45.249.212.51])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 4613E2FC2D;
        Sat,  8 Oct 2022 23:14:12 -0700 (PDT)
Received: from mail02.huawei.com (unknown [172.30.67.153])
        by dggsgout11.his.huawei.com (SkyGuard) with ESMTP id 4MlWsR6RgGzKDcs;
        Sun,  9 Oct 2022 14:11:55 +0800 (CST)
Received: from [10.174.176.73] (unknown [10.174.176.73])
        by APP4 (Coremail) with SMTP id gCh0CgBnJ4etZkJjBhoCCA--.24524S3;
        Sun, 09 Oct 2022 14:14:07 +0800 (CST)
Subject: Re: [LKP] Re: [blk] 8c5035dfbb: fio.read_iops -10.6% regression
To:     Yin Fengwei <fengwei.yin@intel.com>,
        Yu Kuai <yukuai1@huaweicloud.com>,
        kernel test robot <yujie.liu@intel.com>
Cc:     lkp@lists.01.org, lkp@intel.com, Jens Axboe <axboe@kernel.dk>,
        linux-kernel@vger.kernel.org, linux-block@vger.kernel.org,
        "yukuai (C)" <yukuai3@huawei.com>
References: <202210081045.77ddf59b-yujie.liu@intel.com>
 <d5279fc2-38b3-6d20-4404-604d5c7277e2@huaweicloud.com>
 <de2cc259-727a-3707-6738-1c5ab41075a2@intel.com>
From:   Yu Kuai <yukuai1@huaweicloud.com>
Message-ID: <e6d191e7-eda6-2511-1764-221b6f868cc8@huaweicloud.com>
Date:   Sun, 9 Oct 2022 14:14:05 +0800
User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; rv:60.0) Gecko/20100101
 Thunderbird/60.8.0
MIME-Version: 1.0
In-Reply-To: <de2cc259-727a-3707-6738-1c5ab41075a2@intel.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit
X-CM-TRANSID: gCh0CgBnJ4etZkJjBhoCCA--.24524S3
X-Coremail-Antispam: 1UD129KBjvJXoWfGw15WrW5ZryUJFy7ZrWfAFb_yoWkJw4fpr
        n3tFyxJry5Gr1kJr1jyr1UJryUGr1UJ3WUJry8GF18JrWjyF1jgr4UXryqgryDJrW8Ar1U
        Jr15Gr1UZr1UJF7anT9S1TB71UUUUUUqnTZGkaVYY2UrUUUUjbIjqfuFe4nvWSU5nxnvy2
        9KBjDU0xBIdaVrnRJUUUkC14x267AKxVW8JVW5JwAFc2x0x2IEx4CE42xK8VAvwI8IcIk0
        rVWrJVCq3wAFIxvE14AKwVWUJVWUGwA2ocxC64kIII0Yj41l84x0c7CEw4AK67xGY2AK02
        1l84ACjcxK6xIIjxv20xvE14v26F1j6w1UM28EF7xvwVC0I7IYx2IY6xkF7I0E14v26r4U
        JVWxJr1l84ACjcxK6I8E87Iv67AKxVW0oVCq3wA2z4x0Y4vEx4A2jsIEc7CjxVAFwI0_Gc
        CE3s1le2I262IYc4CY6c8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E
        2Ix0cI8IcVAFwI0_Jr0_Jr4lYx0Ex4A2jsIE14v26r1j6r4UMcvjeVCFs4IE7xkEbVWUJV
        W8JwACjcxG0xvEwIxGrwACjI8F5VA0II8E6IAqYI8I648v4I1lc7I2V7IY0VAS07AlzVAY
        IcxG8wCF04k20xvY0x0EwIxGrwCFx2IqxVCFs4IE7xkEbVWUJVW8JwC20s026c02F40E14
        v26r1j6r18MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1VAFwI0_Jw0_GFylIxkG
        c2Ij64vIr41lIxAIcVC0I7IYx2IY67AKxVWUJVWUCwCI42IY6xIIjxv20xvEc7CjxVAFwI
        0_Jr0_Gr1lIxAIcVCF04k26cxKx2IYs7xG6rW3Jr0E3s1lIxAIcVC2z280aVAFwI0_Jr0_
        Gr1lIxAIcVC2z280aVCY1x0267AKxVWUJVW8JbIYCTnIWIevJa73UjIFyTuYvjfUoOJ5UU
        UUU
X-CM-SenderInfo: 51xn3trlr6x35dzhxuhorxvhhfrp/
X-CFilter-Loop: Reflected
X-Spam-Status: No, score=-5.5 required=5.0 tests=BAYES_00,NICE_REPLY_A,
        SPF_HELO_NONE,SPF_PASS autolearn=ham autolearn_force=no version=3.4.6
X-Spam-Checker-Version: SpamAssassin 3.4.6 (2021-04-09) on
        lindbergh.monkeyblade.net
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Hi,

åœ¨ 2022/10/09 13:47, Yin Fengwei å†™é“:
> Hi Kuai,
> 
> On 10/8/22 16:00, Yu Kuai wrote:
>> Hi,
>>
>> åœ¨ 2022/10/08 10:50, kernel test robot å†™é“:
>>> Greeting,
>>>
>>> FYI, we noticed a -10.6% regression of fio.read_iops due to commit:
>>
>> I don't know how this is working but I'm *sure* this commit won't affect
>> performance. Please take a look at the commit, only wbt initialization
>> is touched, which is done while creating the device:
>>
>> device_add_disk
>>  Â blk_register_queue
>>  Â  wbt_enable_default
>>  Â Â  wbt_init
>>
>> And io path is the same with or without this commit.
>>
>> By the way, wbt should only work for write.
> Some information here:
> It looks like the line
>      wbt_set_write_cache(q, test_bit(QUEUE_FLAG_WC, &q->queue_flags));
> matters.
> 
> If move only this line to original position based on 8c5035dfbb,
> the regression is gone.
> 
> If move only this line before ret = rq_qos_add() (just like your patch
> did, but only with this line) based on 8c5035dfbb, the regression can
> be reproduced.
> 

Thanks for the information, but I still don't understand if there is any
difference after wbt_init() is done, and how does read is afftected by
wbt. ðŸ™
> 
> Regards
> Yin, Fengwei
> 
>>
>> Thanks,
>> Kuai
>>>
>>> commit: 8c5035dfbb9475b67c82b3fdb7351236525bf52b ("blk-wbt: call rq_qos_add() after wb_normal is initialized")
>>> https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git master
>>>
>>> in testcase: fio-basic
>>> on test machine: 192 threads 4 sockets Intel(R) Xeon(R) Platinum 9242 CPU @ 2.30GHz (Cascade Lake) with 192G memory
>>> with following parameters:
>>>
>>>  Â Â Â Â runtime: 300s
>>>  Â Â Â Â nr_task: 8t
>>>  Â Â Â Â disk: 1SSD
>>>  Â Â Â Â fs: btrfs
>>>  Â Â Â Â rw: randread
>>>  Â Â Â Â bs: 2M
>>>  Â Â Â Â ioengine: sync
>>>  Â Â Â Â test_size: 256g
>>>  Â Â Â Â cpufreq_governor: performance
>>>
>>> test-description: Fio is a tool that will spawn a number of threads or processes doing a particular type of I/O action as specified by the user.
>>> test-url: https://github.com/axboe/fio
>>>
>>>
>>> Details are as below:
>>>
>>> =========================================================================================
>>> bs/compiler/cpufreq_governor/disk/fs/ioengine/kconfig/nr_task/rootfs/runtime/rw/tbox_group/test_size/testcase:
>>>  Â Â  2M/gcc-11/performance/1SSD/btrfs/sync/x86_64-rhel-8.3/8t/debian-11.1-x86_64-20220510.cgz/300s/randread/lkp-csl-2ap4/256g/fio-basic
>>>
>>> commit:
>>>  Â Â  f7de4886fe ("rnbd-srv: remove struct rnbd_dev")
>>>  Â Â  8c5035dfbb ("blk-wbt: call rq_qos_add() after wb_normal is initialized")
>>>
>>> f7de4886fe8f008a 8c5035dfbb9475b67c82b3fdb73
>>> ---------------- ---------------------------
>>>  Â Â Â Â Â Â Â Â Â  %stddevÂ Â Â Â  %changeÂ Â Â Â Â Â Â Â  %stddev
>>>  Â Â Â Â Â Â Â Â Â Â Â Â Â  \Â Â Â Â Â Â Â Â Â  |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  \
>>>  Â Â Â Â Â Â  0.03 Â±106%Â Â Â Â Â  +0.2Â Â Â Â Â Â Â  0.22 Â± 80%Â  fio.latency_20ms%
>>>  Â Â Â Â Â Â  0.02 Â± 33%Â Â Â Â Â  -0.0Â Â Â Â Â Â Â  0.01 Â± 12%Â  fio.latency_4ms%
>>>  Â Â Â Â Â Â  2508Â Â Â Â Â Â Â Â Â Â  -10.6%Â Â Â Â Â Â  2243Â Â Â Â Â Â Â  fio.read_bw_MBps
>>>  Â Â Â  6717440Â Â Â Â Â Â Â Â Â Â  +17.6%Â Â Â  7897088Â Â Â Â Â Â Â  fio.read_clat_90%_us
>>>  Â Â Â  6892202Â Â Â Â Â Â Â Â Â Â  +19.0%Â Â Â  8202922Â Â Â Â Â Â Â  fio.read_clat_95%_us
>>>  Â Â Â  7602176 Â±Â  4%Â Â Â Â  +18.4%Â Â Â  9000277 Â±Â  3%Â  fio.read_clat_99%_us
>>>  Â Â Â  6374238Â Â Â Â Â Â Â Â Â Â  +11.8%Â Â Â  7127450Â Â Â Â Â Â Â  fio.read_clat_mean_us
>>>  Â Â Â Â  363825 Â± 10%Â Â Â Â  +74.9%Â Â Â Â  636378 Â±Â  5%Â  fio.read_clat_stddev
>>>  Â Â Â Â Â Â  1254Â Â Â Â Â Â Â Â Â Â  -10.6%Â Â Â Â Â Â  1121Â Â Â Â Â Â Â  fio.read_iops
>>>  Â Â Â Â  104.97Â Â Â Â Â Â Â Â Â Â  +11.8%Â Â Â Â  117.32Â Â Â Â Â Â Â  fio.time.elapsed_time
>>>  Â Â Â Â  104.97Â Â Â Â Â Â Â Â Â Â  +11.8%Â Â Â Â  117.32Â Â Â Â Â Â Â  fio.time.elapsed_time.max
>>>  Â Â Â Â Â  13731Â Â Â Â Â Â Â Â Â Â Â  +5.6%Â Â Â Â Â  14498 Â±Â  4%Â  fio.time.maximum_resident_set_size
>>>  Â Â Â Â  116.00Â Â Â Â Â Â Â Â Â Â Â  -8.2%Â Â Â Â  106.50Â Â Â Â Â Â Â  fio.time.percent_of_cpu_this_job_got
>>>  Â  1.998e+10Â Â Â Â Â Â Â Â Â Â  +11.4%Â  2.226e+10Â Â Â Â Â Â Â  cpuidle..time
>>>  Â Â Â Â Â Â  3.27 Â±Â  3%Â Â Â Â Â  +4.6%Â Â Â Â Â Â  3.42Â Â Â Â Â Â Â  iostat.cpu.iowait
>>>  Â Â Â Â Â Â  4.49 Â± 68%Â Â Â Â Â  -2.1Â Â Â Â Â Â Â  2.38 Â±152%Â  perf-profile.children.cycles-pp.number
>>>  Â Â Â Â Â Â  4.49 Â± 68%Â Â Â Â Â  -2.5Â Â Â Â Â Â Â  1.98 Â±175%Â  perf-profile.self.cycles-pp.number
>>>  Â Â Â Â  557763Â Â Â Â Â Â Â Â Â Â Â  +5.4%Â Â Â Â  587781Â Â Â Â Â Â Â  proc-vmstat.pgfault
>>>  Â Â Â Â Â  25488Â Â Â Â Â Â Â Â Â Â Â  +3.1%Â Â Â Â Â  26274Â Â Â Â Â Â Â  proc-vmstat.pgreuse
>>>  Â Â Â  2459048Â Â Â Â Â Â Â Â Â Â  -10.1%Â Â Â  2209482Â Â Â Â Â Â Â  vmstat.io.bi
>>>  Â Â Â Â  184649 Â±Â  5%Â Â Â Â  -10.4%Â Â Â Â  165526 Â±Â  7%Â  vmstat.system.cs
>>>  Â Â Â Â  111733 Â± 30%Â Â Â Â  +61.8%Â Â Â Â  180770 Â± 21%Â  numa-meminfo.node0.AnonPages
>>>  Â Â Â Â  113221 Â± 30%Â Â Â Â  +60.2%Â Â Â Â  181416 Â± 21%Â  numa-meminfo.node0.Inactive(anon)
>>>  Â Â Â Â Â  11301 Â± 24%Â Â Â  +164.5%Â Â Â Â Â  29888 Â±117%Â  numa-meminfo.node2.Active(file)
>>>  Â Â Â Â  104911 Â± 39%Â Â Â Â  -80.5%Â Â Â Â Â  20456 Â±100%Â  numa-meminfo.node3.AnonHugePages
>>>  Â Â Â Â  131666 Â± 27%Â Â Â Â  -67.9%Â Â Â Â Â  42297 Â± 82%Â  numa-meminfo.node3.AnonPages
>>>  Â Â Â Â  132698 Â± 26%Â Â Â Â  -67.5%Â Â Â Â Â  43158 Â± 81%Â  numa-meminfo.node3.Inactive(anon)
>>>  Â Â Â Â Â  27934 Â± 30%Â Â Â Â  +61.8%Â Â Â Â Â  45196 Â± 21%Â  numa-vmstat.node0.nr_anon_pages
>>>  Â Â Â Â Â  28306 Â± 30%Â Â Â Â  +60.2%Â Â Â Â Â  45358 Â± 21%Â  numa-vmstat.node0.nr_inactive_anon
>>>  Â Â Â Â Â  28305 Â± 30%Â Â Â Â  +60.2%Â Â Â Â Â  45357 Â± 21%Â  numa-vmstat.node0.nr_zone_inactive_anon
>>>  Â Â Â Â Â Â  6291 Â± 24%Â Â Â Â  +68.0%Â Â Â Â Â  10567 Â± 26%Â  numa-vmstat.node2.workingset_nodes
>>>  Â Â Â Â Â  32925 Â± 27%Â Â Â Â  -67.9%Â Â Â Â Â  10571 Â± 82%Â  numa-vmstat.node3.nr_anon_pages
>>>  Â Â Â Â Â  33182 Â± 26%Â Â Â Â  -67.5%Â Â Â Â Â  10786 Â± 81%Â  numa-vmstat.node3.nr_inactive_anon
>>>  Â Â Â Â Â  33182 Â± 26%Â Â Â Â  -67.5%Â Â Â Â Â  10786 Â± 81%Â  numa-vmstat.node3.nr_zone_inactive_anon
>>>  Â Â Â Â  161.78 Â±Â  4%Â Â Â Â  -28.2%Â Â Â Â  116.10 Â± 30%Â  sched_debug.cfs_rq:/.runnable_avg.avg
>>>  Â Â Â Â  161.46 Â±Â  4%Â Â Â Â  -28.2%Â Â Â Â  115.85 Â± 30%Â  sched_debug.cfs_rq:/.util_avg.avg
>>>  Â Â Â Â  426382Â Â Â Â Â Â Â Â Â Â  +11.0%Â Â Â Â  473345 Â±Â  6%Â  sched_debug.cpu.clock.avg
>>>  Â Â Â Â  426394Â Â Â Â Â Â Â Â Â Â  +11.0%Â Â Â Â  473357 Â±Â  6%Â  sched_debug.cpu.clock.max
>>>  Â Â Â Â  426370Â Â Â Â Â Â Â Â Â Â  +11.0%Â Â Â Â  473331 Â±Â  6%Â  sched_debug.cpu.clock.min
>>>  Â Â Â Â  426139Â Â Â Â Â Â Â Â Â Â  +10.9%Â Â Â Â  472586 Â±Â  6%Â  sched_debug.cpu.clock_task.avg
>>>  Â Â Â Â  426368Â Â Â Â Â Â Â Â Â Â  +11.0%Â Â Â Â  473130 Â±Â  6%Â  sched_debug.cpu.clock_task.max
>>>  Â Â Â Â  416196Â Â Â Â Â Â Â Â Â Â  +11.1%Â Â Â Â  462228 Â±Â  6%Â  sched_debug.cpu.clock_task.min
>>>  Â Â Â Â Â Â  1156 Â±Â  7%Â Â Â Â  -10.8%Â Â Â Â Â Â  1031 Â±Â  6%Â  sched_debug.cpu.curr->pid.stddev
>>>  Â Â Â Â  426372Â Â Â Â Â Â Â Â Â Â  +11.0%Â Â Â Â  473334 Â±Â  6%Â  sched_debug.cpu_clk
>>>  Â Â Â Â  425355Â Â Â Â Â Â Â Â Â Â  +11.0%Â Â Â Â  472318 Â±Â  6%Â  sched_debug.ktime
>>>  Â Â Â Â  426826Â Â Â Â Â Â Â Â Â Â  +11.0%Â Â Â Â  473787 Â±Â  6%Â  sched_debug.sched_clk
>>>  Â  1.263e+09Â Â Â Â Â Â Â Â Â Â Â  -7.9%Â  1.164e+09 Â±Â  3%Â  perf-stat.i.branch-instructions
>>>  Â Â Â Â  190886 Â±Â  5%Â Â Â Â  -10.8%Â Â Â Â  170290 Â±Â  7%Â  perf-stat.i.context-switches
>>>  Â  1.979e+09Â Â Â Â Â Â Â Â Â Â Â  -8.8%Â  1.804e+09 Â±Â  2%Â  perf-stat.i.dTLB-loads
>>>  Â  8.998e+08Â Â Â Â Â Â Â Â Â Â Â  -8.2%Â  8.257e+08 Â±Â  2%Â  perf-stat.i.dTLB-stores
>>>  Â  6.455e+09Â Â Â Â Â Â Â Â Â Â Â  -8.0%Â  5.938e+09 Â±Â  3%Â  perf-stat.i.instructions
>>>  Â Â Â Â Â  21.78Â Â Â Â Â Â Â Â Â Â Â  -8.4%Â Â Â Â Â  19.95Â Â Â Â Â Â Â  perf-stat.i.metric.M/sec
>>>  Â Â Â  7045315 Â±Â  4%Â Â Â Â  -14.0%Â Â Â  6057863 Â±Â  6%Â  perf-stat.i.node-load-misses
>>>  Â Â Â  2658563 Â±Â  7%Â Â Â Â  -21.9%Â Â Â  2077647 Â± 12%Â  perf-stat.i.node-loads
>>>  Â Â Â Â  414822 Â±Â  4%Â Â Â Â  -12.9%Â Â Â Â  361455 Â±Â  3%Â  perf-stat.i.node-store-misses
>>>  Â  1.251e+09Â Â Â Â Â Â Â Â Â Â Â  -7.8%Â  1.154e+09 Â±Â  3%Â  perf-stat.ps.branch-instructions
>>>  Â Â Â Â  189082 Â±Â  5%Â Â Â Â  -10.7%Â Â Â Â  168849 Â±Â  7%Â  perf-stat.ps.context-switches
>>>  Â Â  1.96e+09Â Â Â Â Â Â Â Â Â Â Â  -8.8%Â  1.789e+09 Â±Â  2%Â  perf-stat.ps.dTLB-loads
>>>  Â  8.912e+08Â Â Â Â Â Â Â Â Â Â Â  -8.1%Â  8.187e+08 Â±Â  2%Â  perf-stat.ps.dTLB-stores
>>>  Â  6.393e+09Â Â Â Â Â Â Â Â Â Â Â  -7.9%Â  5.888e+09 Â±Â  3%Â  perf-stat.ps.instructions
>>>  Â Â Â  6978485 Â±Â  4%Â Â Â Â  -13.9%Â Â Â  6006510 Â±Â  6%Â  perf-stat.ps.node-load-misses
>>>  Â Â Â  2633627 Â±Â  7%Â Â Â Â  -21.8%Â Â Â  2060033 Â± 12%Â  perf-stat.ps.node-loads
>>>  Â Â Â Â  410822 Â±Â  4%Â Â Â Â  -12.8%Â Â Â Â  358289 Â±Â  3%Â  perf-stat.ps.node-store-misses
>>>
>>>
>>> If you fix the issue, kindly add following tag
>>> | Reported-by: kernel test robot <yujie.liu@intel.com>
>>> | Link: https://lore.kernel.org/r/202210081045.77ddf59b-yujie.liu@intel.com
>>>
>>>
>>> To reproduce:
>>>
>>>  Â Â Â Â Â Â Â Â  git clone https://github.com/intel/lkp-tests.git
>>>  Â Â Â Â Â Â Â Â  cd lkp-tests
>>>  Â Â Â Â Â Â Â Â  sudo bin/lkp install job.yamlÂ Â Â Â Â Â Â Â Â Â  # job file is attached in this email
>>>  Â Â Â Â Â Â Â Â  bin/lkp split-job --compatible job.yaml # generate the yaml file for lkp run
>>>  Â Â Â Â Â Â Â Â  sudo bin/lkp run generated-yaml-file
>>>
>>>  Â Â Â Â Â Â Â Â  # if come across any failure that blocks the test,
>>>  Â Â Â Â Â Â Â Â  # please remove ~/.lkp and /lkp dir to run from a clean state.
>>>
>>>
>>> Disclaimer:
>>> Results have been estimated based on internal Intel analysis and are provided
>>> for informational purposes only. Any difference in system hardware or software
>>> design or configuration may affect actual performance.
>>>
>>>
>> _______________________________________________
>> LKP mailing list -- lkp@lists.01.org
>> To unsubscribe send an email to lkp-leave@lists.01.org
> 
> .
> 

