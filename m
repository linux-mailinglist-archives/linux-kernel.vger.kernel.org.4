Return-Path: <linux-kernel-owner@vger.kernel.org>
X-Original-To: lists+linux-kernel@lfdr.de
Delivered-To: lists+linux-kernel@lfdr.de
Received: from out1.vger.email (out1.vger.email [IPv6:2620:137:e000::1:20])
	by mail.lfdr.de (Postfix) with ESMTP id 59D75722DC9
	for <lists+linux-kernel@lfdr.de>; Mon,  5 Jun 2023 19:43:01 +0200 (CEST)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S231191AbjFERm7 (ORCPT <rfc822;lists+linux-kernel@lfdr.de>);
        Mon, 5 Jun 2023 13:42:59 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:53396 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S230150AbjFERm5 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 5 Jun 2023 13:42:57 -0400
Received: from mail-ed1-x52f.google.com (mail-ed1-x52f.google.com [IPv6:2a00:1450:4864:20::52f])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 5C229D3
        for <linux-kernel@vger.kernel.org>; Mon,  5 Jun 2023 10:42:56 -0700 (PDT)
Received: by mail-ed1-x52f.google.com with SMTP id 4fb4d7f45d1cf-5149aafef44so7272855a12.0
        for <linux-kernel@vger.kernel.org>; Mon, 05 Jun 2023 10:42:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20221208; t=1685986974; x=1688578974;
        h=content-transfer-encoding:cc:to:subject:message-id:date:from
         :in-reply-to:references:mime-version:from:to:cc:subject:date
         :message-id:reply-to;
        bh=K1y5t9l7fgNxCwxhJF5oRJyBHW++Rnvru6BGlC3ty3Q=;
        b=JJ2yARfO1j6yctytY7RGFu0N1Ba800EMMeCBuIZlkBQiK8ZPENXew9+W9k1Ln6ixa1
         f5xHogwfGWTkjM0vLDEjQDvPVex34Knt3zl4oI2HhKJkF5nvFCTSr5rAr5H3azXHzFH0
         ZG7umyJv6hMgxgj7dJPrfh/m92oX8PF2Rivuky1jx5LAKQrzv2wr3ZFcl9un79EtV3v7
         8IEmPD4sMP6do2dUecwoo2qCjvTymLWbm1BBUeM7bg417aY/H6KTfPkCVkNMsZzxc6qX
         Ti98DZTq/YM4vn83dqYvBFB/oPOpY/OnvX5wMSQnznEC2UzLt4yQh3OIvHkbP8/gYh9m
         XadA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20221208; t=1685986974; x=1688578974;
        h=content-transfer-encoding:cc:to:subject:message-id:date:from
         :in-reply-to:references:mime-version:x-gm-message-state:from:to:cc
         :subject:date:message-id:reply-to;
        bh=K1y5t9l7fgNxCwxhJF5oRJyBHW++Rnvru6BGlC3ty3Q=;
        b=GuKyECjWpy6REmXkH+kY6fk8iEWyeu5GQll9K/3JC7QcTA1qokNSUlaA2+naKYDVZ4
         xcjF9JFmYQbkb8OPDVINNcFkMUkzDT6vEO+qYocMniGWE4823Z19FqQL2qtnOAbxFySA
         jJXDociCt/To5RYObPAqSXbgTZmczd0qirnigT729gDRZTzY8vJFBbj/MjmK107VTS+A
         DDvsvocF3DbbE6KqVcnhU9IOMwUMdoHOdqRr7kL6ojoMDcFo39wfwFcS4AZVPNMuRvL6
         Dt4mD1NjBRN/siIM+XfILLCr3wfAqxk7SmD3gWSpvOv1Fl7hNI5hVz6gjzyVe3ZaKPQ5
         Zzdg==
X-Gm-Message-State: AC+VfDxI+Gh/20bMhB8Q5VubBHa1ev5wYNrgNHEMmajtrEj4Ev6Pa7Eh
        2xdYJ2+ob55k9rbyPU0KsYqiuJbN58UJICbO6/NTjA==
X-Google-Smtp-Source: ACHHUZ5rd4wnlUPX3vnRn8UOpfyeh0jb4JFLgcyiQf2uwLDSz3fu9nRd5EsNEz+1nStKKC3LUh3M9sS1XTOwTmirexI=
X-Received: by 2002:a17:906:58cf:b0:96a:1c2a:5a38 with SMTP id
 e15-20020a17090658cf00b0096a1c2a5a38mr6459925ejs.11.1685986974517; Mon, 05
 Jun 2023 10:42:54 -0700 (PDT)
MIME-Version: 1.0
References: <20230605004334.1930091-1-mizhang@google.com> <CALMp9eSQgcKd=SN4q2QRYbveKoayKzuYEQPM0Xu+FgQ_Mja8-g@mail.gmail.com>
In-Reply-To: <CALMp9eSQgcKd=SN4q2QRYbveKoayKzuYEQPM0Xu+FgQ_Mja8-g@mail.gmail.com>
From:   Mingwei Zhang <mizhang@google.com>
Date:   Mon, 5 Jun 2023 10:42:18 -0700
Message-ID: <CAL715WJowYL=W40SWmtPoz1F9WVBFDG7TQwbsV2Bwf9-cS77=Q@mail.gmail.com>
Subject: Re: [PATCH] KVM: x86/mmu: Remove KVM MMU write lock when accessing indirect_shadow_pages
To:     Jim Mattson <jmattson@google.com>
Cc:     Sean Christopherson <seanjc@google.com>,
        Paolo Bonzini <pbonzini@redhat.com>,
        "H. Peter Anvin" <hpa@zytor.com>, kvm@vger.kernel.org,
        linux-kernel@vger.kernel.org, Ben Gardon <bgardon@google.com>
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable
X-Spam-Status: No, score=-17.6 required=5.0 tests=BAYES_00,DKIMWL_WL_MED,
        DKIM_SIGNED,DKIM_VALID,DKIM_VALID_AU,DKIM_VALID_EF,
        ENV_AND_HDR_SPF_MATCH,RCVD_IN_DNSWL_NONE,SPF_HELO_NONE,SPF_PASS,
        T_SCC_BODY_TEXT_LINE,USER_IN_DEF_DKIM_WL,USER_IN_DEF_SPF_WL
        autolearn=unavailable autolearn_force=no version=3.4.6
X-Spam-Checker-Version: SpamAssassin 3.4.6 (2021-04-09) on
        lindbergh.monkeyblade.net
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Mon, Jun 5, 2023 at 9:55=E2=80=AFAM Jim Mattson <jmattson@google.com> wr=
ote:
>
> On Sun, Jun 4, 2023 at 5:43=E2=80=AFPM Mingwei Zhang <mizhang@google.com>=
 wrote:
> >
> > Remove KVM MMU write lock when accessing indirect_shadow_pages counter =
when
> > page role is direct because this counter value is used as a coarse-grai=
ned
> > heuristics to check if there is nested guest active. Racing with this
> > heuristics without mmu lock will be harmless because the corresponding
> > indirect shadow sptes for the GPA will either be zapped by this thread =
or
> > some other thread who has previously zapped all indirect shadow pages a=
nd
> > makes the value to 0.
> >
> > Because of that, remove the KVM MMU write lock pair to potentially redu=
ce
> > the lock contension and improve the performance of nested VM. In additi=
on
> > opportunistically change the comment of 'direct mmu' to make the
> > description consistent with other places.
> >
> > Reported-by: Jim Mattson <jmattson@google.com>
> > Signed-off-by: Mingwei Zhang <mizhang@google.com>
> > ---
> >  arch/x86/kvm/x86.c | 10 ++--------
> >  1 file changed, 2 insertions(+), 8 deletions(-)
> >
> > diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
> > index 5ad55ef71433..97cfa5a00ff2 100644
> > --- a/arch/x86/kvm/x86.c
> > +++ b/arch/x86/kvm/x86.c
> > @@ -8585,15 +8585,9 @@ static bool reexecute_instruction(struct kvm_vcp=
u *vcpu, gpa_t cr2_or_gpa,
> >
> >         kvm_release_pfn_clean(pfn);
> >
> > -       /* The instructions are well-emulated on direct mmu. */
> > +       /* The instructions are well-emulated on Direct MMUs. */
> >         if (vcpu->arch.mmu->root_role.direct) {
> > -               unsigned int indirect_shadow_pages;
> > -
> > -               write_lock(&vcpu->kvm->mmu_lock);
> > -               indirect_shadow_pages =3D vcpu->kvm->arch.indirect_shad=
ow_pages;
> > -               write_unlock(&vcpu->kvm->mmu_lock);
> > -
> > -               if (indirect_shadow_pages)
> > +               if (READ_ONCE(vcpu->kvm->arch.indirect_shadow_pages))
>
> I don't understand the need for READ_ONCE() here. That implies that
> there is something tricky going on, and I don't think that's the case.

READ_ONCE() is just telling the compiler not to remove the read. Since
this is reading a global variable,  the compiler might just read a
previous copy if the value has already been read into a local
variable. But that is not the case here...

Note I see there is another READ_ONCE for
kvm->arch.indirect_shadow_pages, so I am reusing the same thing.

I did check the reordering issue but it should be fine because when
'we' see indirect_shadow_pages as 0, the shadow pages must have
already been zapped. Not only because of the locking, but also the
program order in __kvm_mmu_prepare_zap_page() shows that it will zap
shadow pages first before updating the stats.
